{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "import scipy.fftpack as fp\n",
    "import collections\n",
    "import cv2\n",
    "from __future__ import print_function\n",
    "from pprint import pprint\n",
    "import math\n",
    "from numpy import binary_repr\n",
    "from skimage import color, data, restoration\n",
    "from scipy.fftpack import fftn, ifftn, fftshift\n",
    "from skimage import img_as_float"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 1\n",
    "\n",
    "<b>Describe, in your own words, what are sampling and quantization. Given the function $f(x) = -0.5x^2 + 3.5x + 1$, generate the binary code that represents the digitized signal using a sampling rate of 0.5 units in the interval [0, 5] and 16 grey levels. Consider that the grey levels 0 and 15 are equal to the function values 0 and 7.5, respectively.</b>\n",
    "\n",
    "Sampling and quantization are related to the capture of images through sensors like cameras. The sampling is the mapping of coordinate and quantization is the attribution of value of that coordinate.\n",
    "\n",
    "For example, we gonna start the capture of one image through a single sensor that identify one color in grayscale. We could define that image will be 8x8 shape. In this case, the sensor will verify the first position, this is the *sampling* procedure, the sensor is mapping or digitalizing that position [0,0] will have a value. The sampling always has to be equally spaced between the samples. Then, we perform the *quantization*, where the sensor will generate a signal related to the value that will be storage to that coordinate. The signal can vary in the format. In this example the signal can be from 0 to 1024, where we get 400 at this position, so it's required scale this value to 0-255 scale, in this case we gonna storage the 99 value at [0,0] coordinate. This procedure is repeated for all coordinates.\n",
    "\n",
    "The function **exFunc** from next code is the implementation of the equation $f(x) = -0.5x^2 + 3.5x + 1$, the for instruction call the function in the interval of x = [0,5]. The function **rescale15** take the result and rescale in 16 levels of grayscale. Then, the binary code is printed along with a chart representing the values.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def exFunc(x):\n",
    "    return -0.5*(x**2) + 3.5 * x + 1\n",
    "\n",
    "def rescale15(v):\n",
    "    return int(round(v*15/7.5))\n",
    "\n",
    "arr = []\n",
    "print(\"Signal\\tScale\\tBin\")\n",
    "for v in np.arange(0,5,0.5):\n",
    "    nV = exFunc(v)\n",
    "    res = rescale15(nV)\n",
    "    arr.append(res)\n",
    "    print(nV,\"\\t\", res, \"\\t\",binary_repr(res,4))\n",
    "    \n",
    "plt.title(\"Signal in scale 0-15\")\n",
    "plt.plot(arr)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 2\n",
    "<b>Implement, from scratch, convolution on the spatial domain. Your function should receive the kernel and its dimensions as parameters. Considering that the kernel is square and has odd dimensions d, ignore the first and last d=2 rows and columns. Use it to perform low-pass and high-pass filtering on some images.</b>\n",
    "\n",
    "To solve this question I created the function **spatialFilter** that basically receive the image, the kernel and the size of kernel to be apply on the image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def spatialFilter(img, kernel, kernel_size):    \n",
    "    new_img = np.zeros_like(img)\n",
    "    divided = math.floor(kernel_size/2)\n",
    "    \n",
    "    for i in range(divided,img.shape[0]-divided,1):\n",
    "        for j in range(divided,img.shape[1]-divided,1):\n",
    "            space = img[i-divided:i+divided+1,j-divided:j+divided+1]\n",
    "            new_img[i,j] = np.sum(space*kernel)/ (kernel_size*kernel_size)\n",
    "            \n",
    "    return new_img\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Low Pass Filter\n",
    "\n",
    "To illustrate the application of Low Pass Filter, I created the function **lowPassFilter** where only is needed to pass the image. Inside this function is defined one kernel with 15x15, and it will apply my filter and the OpenCV filter for comparison. So I decided to test in the mig.jpg image and in the image that the book of course  uses to realize these tests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lowPassFilter(img):\n",
    "\n",
    "    fig, ax = plt.subplots(1, 3)\n",
    "    fig.set_size_inches(15, 10, forward=True)\n",
    "    ax[0].imshow(img, cmap=plt.cm.gray)\n",
    "    ax[0].title.set_text(\"Original\")\n",
    "\n",
    "    kernel = np.ones((15,15))\n",
    "    \n",
    "    img2 = spatialFilter(img, kernel=kernel, kernel_size=kernel.shape[0])\n",
    "    ax[1].imshow(img2, cmap=plt.cm.gray)\n",
    "    ax[1].title.set_text(\"My blur\")\n",
    "    \n",
    "    \n",
    "    img2 = cv2.blur(img,(15,15),0)\n",
    "    ax[2].imshow(img2, cmap=plt.cm.gray)\n",
    "    ax[2].title.set_text(\"OpenCV blur\")\n",
    "    plt.show()\n",
    "    \n",
    "\n",
    "img = cv2.imread('mit.jpg', 0)\n",
    "lowPassFilter(img)\n",
    "\n",
    "img = cv2.imread('a.png', 0)\n",
    "lowPassFilter(img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### High Pass Filter\n",
    "\n",
    "To illustrate the *High Pass Filter*, I created four different kernels to be applied through my function *spatialFilter* and the OpenCV version."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def highPassFilter(img):   \n",
    "    fig, ax = plt.subplots(1, 2)\n",
    "    fig.set_size_inches(10, 15, forward=True)\n",
    "    \n",
    "    \n",
    "    #kernel 1\n",
    "    kernel = np.array([[0, 1, 0],\n",
    "                       [1, -4, 1],\n",
    "                       [0, 1, 0]])\n",
    "    \n",
    "    #essa divisão pelo shape só pode ser feita na minha funcao, a funcao do OpenCv aparentemente já faz isso\n",
    "    #e ela só serve no high pass, nao pode ser usada no low pass\n",
    "    k = kernel/(kernel.shape[0]*kernel.shape[1])\n",
    "    \n",
    "    img2 = spatialFilter(img, kernel=k, kernel_size=kernel.shape[0])\n",
    "    ax[0].imshow(img2, cmap=plt.cm.gray)\n",
    "    ax[0].title.set_text(\"My high pass kernel 1\")\n",
    "    \n",
    "    img2 = cv2.filter2D(img,-20,kernel)\n",
    "    ax[1].imshow(img2, cmap=plt.cm.gray)\n",
    "    ax[1].title.set_text(\"OpenCV HighPass\")\n",
    "    \n",
    "    \n",
    "    fig, ax = plt.subplots(1, 2)\n",
    "    fig.set_size_inches(10, 15, forward=True)\n",
    "    #kernel 2\n",
    "    kernel = np.array([[1,1,1], \n",
    "                       [1,-8,1],\n",
    "                       [1,1,1]])\n",
    "    \n",
    "    k = kernel/(kernel.shape[0]*kernel.shape[1])\n",
    "    \n",
    "    img2 = spatialFilter(img, kernel=k, kernel_size=kernel.shape[0])\n",
    "    ax[0].imshow(img2, cmap=plt.cm.gray)\n",
    "    ax[0].title.set_text(\"My high pass kernel 2\")\n",
    "    \n",
    "    img2 = cv2.filter2D(img,-1,kernel)\n",
    "    ax[1].imshow(img2, cmap=plt.cm.gray)\n",
    "    ax[1].title.set_text(\"OpenCV HighPass\")\n",
    "    \n",
    "    fig, ax = plt.subplots(1, 2)\n",
    "    fig.set_size_inches(10, 15, forward=True)\n",
    "    #kernel 3\n",
    "    kernel = np.array([[0, -1, 0],\n",
    "                       [-1, 4, -1],\n",
    "                       [0, -1, 0]])    \n",
    "    \n",
    "    k = kernel/(kernel.shape[0]*kernel.shape[1])\n",
    "    \n",
    "    \n",
    "    img2 = spatialFilter(img, kernel=k, kernel_size=kernel.shape[0])\n",
    "    ax[0].imshow(img2, cmap=plt.cm.gray)\n",
    "    ax[0].title.set_text(\"My high pass kernel 3\")\n",
    "    \n",
    "    img2 = cv2.filter2D(img,-1,kernel)\n",
    "    ax[1].imshow(img2, cmap=plt.cm.gray)\n",
    "    ax[1].title.set_text(\"OpenCV HighPass\")\n",
    "    \n",
    "    fig, ax = plt.subplots(1, 2)\n",
    "    fig.set_size_inches(10, 15, forward=True)\n",
    "    #kernel 4\n",
    "    kernel = np.array([[-1,-1,-1], \n",
    "                       [-1,8,-1],\n",
    "                       [-1,-1,-1]])\n",
    "    \n",
    "    k = kernel/(kernel.shape[0]*kernel.shape[1])\n",
    "    \n",
    "    img2 = spatialFilter(img, kernel=k, kernel_size=kernel.shape[0])\n",
    "    ax[0].imshow(img2, cmap=plt.cm.gray)\n",
    "    ax[0].title.set_text(\"My high pass kernel 4\")\n",
    "    \n",
    "    img2 = cv2.filter2D(img,-1,kernel)\n",
    "    ax[1].imshow(img2, cmap=plt.cm.gray)\n",
    "    ax[1].title.set_text(\"OpenCV HighPass\")\n",
    "    \n",
    "    \n",
    "    plt.show()\n",
    "    \n",
    "    \n",
    "img = cv2.imread('mit.jpg', 0)\n",
    "highPassFilter(img)\n",
    "\n",
    "img = cv2.imread('a.png', 0)\n",
    "highPassFilter(img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 3\n",
    "\n",
    "<b>Implement, from scratch, the histogram equalization technique and apply it to the images of the test set with low contrast.</b>\n",
    "\n",
    "To solve this question I created four functions:\n",
    "\n",
    "- **histTable** to generate the histogram in Dict format (Dict is the python hashtable)\n",
    "- **getCdf** to calculate the *Cumulative distribution*\n",
    "- **equalized** that can receive one image and call the previous functions to generate the equalization table or receive that personalized equalization table. (I decided to create this function in this way because I was doing others tests from the book)\n",
    "- **equalize** finally the function to apply the equalization in one image.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Gera o histograma\n",
    "#Uma lista com a distribuição de todos os tons presentes na imagem\n",
    "#E a quantidade de vezes que a tonalidade está presente\n",
    "def histTable(img):\n",
    "    hist = {}\n",
    "    for i in range(img.shape[0]):\n",
    "        for j in range(img.shape[1]):\n",
    "            color = img[i,j]\n",
    "            \n",
    "            if color in hist.keys():\n",
    "                hist[color] += 1\n",
    "            else:\n",
    "                hist[color] = 1\n",
    "    #Tem que ser ordenado\n",
    "    return collections.OrderedDict(sorted(hist.items()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cálculo da Cumulative distribuition\n",
    "#Para cada tonaliade é somada a sua quantidade com a quantidade de tonalidades anteriores\n",
    "#https://en.wikipedia.org/wiki/Histogram_equalization\n",
    "def getCdf(img,hist=None):\n",
    "    if hist == None:\n",
    "        hist = histTable(img)\n",
    "    \n",
    "    cdf = {}\n",
    "    for i in hist:\n",
    "        cdf[i] = 0\n",
    "        for j in hist:\n",
    "            cdf[i] += hist[j]\n",
    "            if j == i:\n",
    "                break\n",
    "    \n",
    "    return cdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Gera a tabela com os valores equalizados\n",
    "#A chave da tabela é o valor atual da tonalidade e o valor é a nova tonalidade\n",
    "def equalized(img,hist=None,space=256):\n",
    "    cdf = getCdf(img,hist)    \n",
    "    _minCdf = cdf[min(cdf)]\n",
    "    \n",
    "    eq = {}\n",
    "    for i in cdf:\n",
    "        try:\n",
    "            r = round((cdf[i] - _minCdf)/ (img.shape[0]*img.shape[1] - _minCdf) * space-1)\n",
    "        except:\n",
    "            r = 0\n",
    "        eq[i] = r\n",
    "    return eq\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Aplica a equalização nas imagens\n",
    "def equalize(img,hist=None,space=256):\n",
    "    new = np.zeros_like(img)\n",
    "    eq = equalized(img,hist,space)\n",
    "    for i in range(img.shape[0]):\n",
    "        for j in range(img.shape[1]):\n",
    "            color = img[i,j]\n",
    "            \n",
    "            if color in eq.keys():\n",
    "                new[i,j] = eq[color]\n",
    "    return new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = cv2.imread('Google_JAX_low_contrast.jpg', 0)\n",
    "fig, ax = plt.subplots(2, 3)\n",
    "fig.set_size_inches(15, 10, forward=True)\n",
    "pos = [(j,i) for j in range(4) for i in range(3)]\n",
    "p = 0\n",
    "\n",
    "#A\n",
    "ax[pos[p]].imshow(img, cmap=plt.cm.gray, vmin=0, vmax=255)\n",
    "p += 1\n",
    "\n",
    "img2 = equalize(img)\n",
    "ax[pos[p]].imshow(img2, cmap=plt.cm.gray, vmin=0, vmax=255)\n",
    "p += 1\n",
    "\n",
    "histr = cv2.calcHist([img2], [0], None, [256], [0, 256])\n",
    "histr_orig = cv2.calcHist([img], [0], None, [256], [0, 256])\n",
    "ax[pos[p]].plot(histr,label='equalized')\n",
    "ax[pos[p]].plot(histr_orig,label='original')\n",
    "ax[pos[p]].axis(xmin=0,xmax=256)\n",
    "ax[pos[p]].legend(loc='upper right')\n",
    "p += 1\n",
    "\n",
    "#B\n",
    "img = cv2.imread('woman_low_contrast.jpg', 0)\n",
    "ax[pos[p]].imshow(img, cmap=plt.cm.gray, vmin=0, vmax=255)\n",
    "p += 1\n",
    "\n",
    "img2 = equalize(img)\n",
    "ax[pos[p]].imshow(img2, cmap=plt.cm.gray, vmin=0, vmax=255)\n",
    "p += 1\n",
    "\n",
    "histr = cv2.calcHist([img2], [0], None, [256], [0, 256])\n",
    "histr_orig = cv2.calcHist([img], [0], None, [256], [0, 256])\n",
    "ax[pos[p]].plot(histr,label='equalized')\n",
    "ax[pos[p]].plot(histr_orig,label='original')\n",
    "ax[pos[p]].axis(xmin=0,xmax=256)\n",
    "ax[pos[p]].legend(loc='upper right')\n",
    "p += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 4\n",
    "<b>Implement, from scratch, the median filter and apply it to the images with\n",
    "salt-and-pepper noise.</b>\n",
    "\n",
    "To solve this question I implemented the function *medianFilter* similar to *spatialFilter*, but in this one has the calculation of median in each image subarea. Then I applied this function on the images through the function *applyMedian*, that will call my function and the OpenCV function to comparison."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def medianFilter(img, kernel_size):\n",
    "        \n",
    "    new_img = np.zeros_like(img)\n",
    "    divided = math.floor(kernel_size/2)\n",
    "    \n",
    "    for i in range(divided,img.shape[0]-divided,1):\n",
    "        for j in range(divided,img.shape[1]-divided,1):\n",
    "            space = img[i-divided:i+divided+1,j-divided:j+divided+1]\n",
    "            new_img[i,j] = np.median(space)\n",
    "            \n",
    "    return new_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def applyMedian(img, name=\"\"):\n",
    "    fig, ax = plt.subplots(1, 3)\n",
    "    fig.set_size_inches(15, 10)\n",
    "\n",
    "    my_median = medianFilter(img,7)\n",
    "    median = cv2.medianBlur(img,7)\n",
    "\n",
    "    ax[0].imshow(img, cmap=plt.cm.gray)\n",
    "    ax[0].title.set_text('Original %s' % name)\n",
    "    \n",
    "    ax[1].imshow(my_median, cmap=plt.cm.gray)\n",
    "    ax[1].title.set_text('My median')\n",
    "    \n",
    "    ax[2].imshow(median, cmap=plt.cm.gray)\n",
    "    ax[2].title.set_text('OpenCV Median')\n",
    "    plt.show()\n",
    "    \n",
    "\n",
    "img = cv2.imread('bear_s_and_p.png', 0)\n",
    "applyMedian(img,'bear_s_and_p')\n",
    "img = cv2.imread('boat_s_and_p.png', 0)\n",
    "applyMedian(img,'boat_s_and_p')\n",
    "img = cv2.imread('lions_s_and_p.png', 0)\n",
    "applyMedian(img,'lions_s_and_p')\n",
    "img = cv2.imread('glass_s_and_p.png', 0)\n",
    "applyMedian(img,'glass_s_and_p')\n",
    "img = cv2.imread('lungs_s_and_p_10.jpg', 0)\n",
    "applyMedian(img,'lungs_s_and_p_10')\n",
    "img = cv2.imread('lungs_s_and_p_30.jpg', 0)\n",
    "applyMedian(img,'lungs_s_and_p_30')\n",
    "img = cv2.imread('lungs_s_and_p_50.jpg', 0)\n",
    "applyMedian(img,'lungs_s_and_p_50')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The book example\n",
    "It is worth noting that, the book presents this example, where the author says that was applied the median filter on the image A, and he got the image D, but I could't perform the same effect, neither with my function nor with OpenCV."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(2, 2)\n",
    "fig.set_size_inches(10, 10)\n",
    "\n",
    "\n",
    "img = cv2.imread('16-circuit-salt-and-pepper.png', 0)\n",
    "\n",
    "my_median = medianFilter(img,7)\n",
    "median = cv2.medianBlur(img,7)\n",
    "\n",
    "ax[0,0].imshow(img, cmap=plt.cm.gray)\n",
    "ax[0,0].title.set_text('Original A')\n",
    "\n",
    "ax[0,1].imshow(my_median, cmap=plt.cm.gray)\n",
    "ax[0,1].title.set_text('My median B')\n",
    "\n",
    "ax[1,0].imshow(median, cmap=plt.cm.gray),\n",
    "ax[1,0].title.set_text('OpenCV Median C')\n",
    "\n",
    "img = cv2.imread('16-circuit-salt-and-pepper-median.png', 0)\n",
    "ax[1,1].imshow(img, cmap=plt.cm.gray),\n",
    "ax[1,1].title.set_text('Expectation D')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 5\n",
    "**Using a Fourier transform library, implement the high-frequency emphasis\n",
    "filter defined by:**\n",
    "\n",
    "$$g(x,y) = \\mathcal{F}^{-1}{[k_1 + k_2 H_{HP} (u, v)]F(u, v)}$$\n",
    "\n",
    "**where $k_1 \\leq 0$ offsets the value the transfer function so as not to zero-out\n",
    "the dc term, and $k_2 > 0$ controls the contribution of high frequencies.\n",
    "Process the image ''full body PET - original.jpg'', varying the values of $k_1$\n",
    "and $k_2$ and select the best values, in your opinion.**\n",
    "\n",
    "To solve this question I believe that I made one implementation slightly different, cause I implemented the function **filterMask** that will generate the mask of types IDEAL, BUTTERWORTH or GAUSSIAN, I only need to pass the shape, type and if will be an LOW_PASS or HIGH_PASS mask."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask_types = [\"IDEAL_MASK\",\"BUTTERWORTH_MASK\",\"GAUSSIAN_MASK\"]\n",
    "IDEAL_MASK = 0\n",
    "BUTTERWORTH_MASK = 1\n",
    "GAUSSIAN_MASK = 2\n",
    "LOW_PASS = 0\n",
    "HIGH_PASS = 1\n",
    "\n",
    "#IMPLEMENTACAO CORRETA\n",
    "#mask_type=IDEAL_MASK = 0\n",
    "#mask_type=BUTTERWORTH_MASK = 1\n",
    "#mode=LOW_PASS\n",
    "#mode=HIGH_PASS\n",
    "def filterMask(dim,D0=None,n=1,mask_type=IDEAL_MASK,mode=LOW_PASS):\n",
    "    \n",
    "    if D0 == None:\n",
    "        D0 = math.floor(dim[0]*0.5)\n",
    "        if mask_type == GAUSSIAN_MASK:\n",
    "            D0 = 0.05\n",
    "        \n",
    "    mask = np.zeros(dim)\n",
    "    \n",
    "    \n",
    "    for u in range(dim[0]):\n",
    "        for v in range(dim[1]):\n",
    "            #Esta implementação está errada!\n",
    "            #Duv = math.sqrt(u**2+v**2)\n",
    "            #Essa daqui foi dificil, em cada lugar ta de um jeito diferente\n",
    "            Duv = (dim[0]/2.0 - u)**2 + (dim[1]/2.0 - v)**2\n",
    "            \n",
    "            if mask_type==IDEAL_MASK:\n",
    "                if Duv > D0**2:\n",
    "                    mask[u,v] = 0\n",
    "                elif Duv <= D0**2:\n",
    "                    mask[u,v] = 1\n",
    "            elif mask_type == GAUSSIAN_MASK:\n",
    "                mask[u,v] = math.exp( -Duv**2/2*D0**2 )\n",
    "            else:\n",
    "                mask[u,v] = (1 / (1 + ((math.sqrt(2)-1) * Duv/D0))**(2*n))\n",
    "    if (mode==HIGH_PASS):\n",
    "        ones = np.ones_like(mask)\n",
    "        mask = ones - mask\n",
    "        \n",
    "    return mask\n",
    "\n",
    "fig, ax = plt.subplots(1, 3)\n",
    "fig.set_size_inches(7, 5)\n",
    "mask = filterMask((30,30),mode=HIGH_PASS)\n",
    "ax[0].imshow(mask, cmap = 'gray')\n",
    "ax[0].title.set_text(\"IDEAL\")\n",
    "\n",
    "mask = filterMask((30,30),mask_type=GAUSSIAN_MASK,mode=HIGH_PASS)\n",
    "ax[1].imshow(mask, cmap = 'gray')\n",
    "ax[1].title.set_text(\"GAUSSIAN\")\n",
    "\n",
    "mask = filterMask((30,30), mask_type=BUTTERWORTH_MASK,mode=HIGH_PASS)\n",
    "ax[2].imshow(mask, cmap = 'gray')\n",
    "ax[2].title.set_text(\"BUTTERWORTH_MASK\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then I implemented the function that will apply the mask in many configurations on the image:\n",
    "- **getMag** Fourier transform\n",
    "- **inverse** Inverse Fourier transform\n",
    "- **spectrum** Applies the log on the spectrum to improve the visualization\n",
    "- **rescale255** Rescale the matrix values to 0-255\n",
    "- **runHPTests** Applies many configurations of the selected mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def getMag(img):\n",
    "    F1 = fp.fft2((img).astype(float))\n",
    "    F2 = fp.fftshift(F1)\n",
    "    return F2\n",
    "\n",
    "def inverse(mag):\n",
    "    return fp.ifft2(fp.ifftshift(mag)).real\n",
    "\n",
    "def spectrum(mag):\n",
    "    return (20*np.log10( 0.1 + mag)).astype(int)\n",
    "\n",
    "\n",
    "def rescale255(arr):\n",
    "    div = (arr.max() - arr.min())\n",
    "    #print (div)\n",
    "    if div == 0:\n",
    "        div = 1\n",
    "    return ((arr - arr.min()) * (1/div * 255)).astype('uint8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def runHPTests(img,D0,mask_type):\n",
    "    \n",
    "    mag = getMag(img)\n",
    "    mag_mod = mag.copy()\n",
    "\n",
    "    step = 0.40\n",
    "    mask_orig = filterMask(mag_mod.shape,\n",
    "                            D0=D0,\n",
    "                           mask_type=mask_type,\n",
    "                           mode=HIGH_PASS)\n",
    "\n",
    "    mag_mod = mask_orig * mag_mod\n",
    "    inv = inverse(mag_mod)\n",
    "\n",
    "\n",
    "    fig, ax = plt.subplots(1, 3)\n",
    "    fig.set_size_inches(18, 10)\n",
    "    fig.suptitle(\"%s D0=%d\" % (mask_types[mask_type],D0))\n",
    "    ax[0].imshow( img, cmap='gray')\n",
    "    ax[0].title.set_text(\"Original\")\n",
    "    ax[1].imshow( mask_orig, cmap='gray')\n",
    "    ax[1].title.set_text(\"Mask\")\n",
    "    ax[2].imshow( spectrum(mag_mod), cmap='gray')\n",
    "    ax[2].title.set_text(\"Spectrum\")\n",
    "    plt.show()\n",
    "    \n",
    "    \n",
    "    iteracao = 0\n",
    "\n",
    "    for k2 in np.arange(0.0, 1.0+step, step):\n",
    "        for k1 in np.arange(0.0, 1.0+step, step):\n",
    "            iteracao += 1\n",
    "            \n",
    "            fig, ax = plt.subplots(1, 4)\n",
    "            fig.set_size_inches(18, 7)\n",
    "            fig.suptitle(\"ITERATION %d %s D0=%d k1=%0.2f k2=%0.2f\" % (iteracao,mask_types[mask_type],D0,k1,k2))\n",
    "\n",
    "            mag_mod = mag.copy()\n",
    "            mask = mask_orig.copy()\n",
    "            mag_mod = (k1 + k2 * mask) * mag_mod\n",
    "            inv = inverse(mag_mod)\n",
    "            uni = img+inv\n",
    "\n",
    "            \n",
    "            ax[0].imshow( inv, cmap='gray')\n",
    "            ax[0].axis('off')\n",
    "            ax[0].title.set_text('Inverse')\n",
    "            \n",
    "            ax[1].imshow( uni, cmap='gray')\n",
    "            ax[1].axis('off')\n",
    "            ax[1].title.set_text('Inverse+Orig')\n",
    "\n",
    "            inv = rescale255(np.around(inv))\n",
    "            uni = rescale255(np.around(uni))\n",
    "\n",
    "            \n",
    "            ax[2].imshow( equalize(inv), cmap='gray',vmin=0,vmax=255)\n",
    "            ax[2].title.set_text('Inverse Equalized')\n",
    "            ax[2].axis('off')\n",
    "            \n",
    "            ax[3].imshow( inv, cmap='gray',vmin=0,vmax=170)\n",
    "            ax[3].title.set_text('Inverse vmax=170')\n",
    "            ax[3].axis('off')\n",
    "\n",
    "            plt.show()\n",
    "            \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first image is the original, the second is the mask that will be applied, and the third is the result of that mask on the spectrum. The tested parameters modify the mask and consequentily modifies the spectrum, but the difference in the spectrum is almost imperceptible, for this reason I decided not reprint the spectrum.\n",
    "\n",
    "\n",
    "Then I show the used configurations, ex: IDEAL_MASK D0=5 k1=0.00 k2=0.00 with the results.\n",
    "\n",
    "- **Inverse** is the inverse Fourier transform after applying the mask\n",
    "- **Inverse+Orig** is the sum of inverse with the original image\n",
    "- **Inverse equalized** I noticed that on the figure 4.59 on the 3° ed of the book, has the equalized version of inverse, so i decided show this same equalization, but I was not satisfied with the result.\n",
    "- **Inverse vmax=150** As I was not satisfied with the result of equalization, I decided to show the inverse with the [0-150] scale option instead of [0-255]. This generated a better result.\n",
    "\n",
    "\n",
    "Looking to Inverse vmax=170, on the ninth iteration, D0=5, k1=0 and k2=0.80, with IDEAL_MASK or thirteenth iteration, D0=300, k1=0 and k2=1.2, with BUTTERWORTH_MASK or GAUSSIAN_MASK, is possible to see more clearly eight tiny points in the brain, beyond the big white point in the brain and two big white points in the right lung. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mag = getMag(cv2.imread('pet.jpg',0))\n",
    "mask = filterMask(mag.shape,D0=5,\n",
    "                       mask_type=IDEAL_MASK,\n",
    "                       mode=HIGH_PASS)\n",
    "mask2 = filterMask(mag.shape,D0=300,\n",
    "                       mask_type=BUTTERWORTH_MASK,\n",
    "                       mode=HIGH_PASS)\n",
    "\n",
    "res1 = inverse((0 + 0.8 * mask) * mag)\n",
    "res2 = inverse((0 + 1.2 * mask2) * mag)\n",
    "\n",
    "fig, ax = plt.subplots(1, 2)\n",
    "fig.set_size_inches(15, 10)\n",
    "\n",
    "circles = []\n",
    "circles.append(plt.Circle((125, 48), 5, color='w', fill=False))\n",
    "circles.append(plt.Circle((130, 50), 5, color='w', fill=False))\n",
    "circles.append(plt.Circle((133, 45), 5, color='w', fill=False))\n",
    "circles.append(plt.Circle((140, 35), 5, color='w', fill=False))\n",
    "circles.append(plt.Circle((145, 40), 5, color='w', fill=False))\n",
    "circles.append(plt.Circle((146, 50), 5, color='w', fill=False))\n",
    "circles.append(plt.Circle((153, 55), 5, color='w', fill=False))\n",
    "circles.append(plt.Circle((130, 180), 12, color='w', fill=False))\n",
    "circles.append(plt.Circle((110, 165), 12, color='w', fill=False))\n",
    "\n",
    "for c in circles:\n",
    "    ax[0].add_patch(c)\n",
    "\n",
    "ax[0].imshow(res1,cmap='gray',vmax=170)\n",
    "ax[0].title.set_text(\"Inverse vmax=170 9° ITER D0=5 k1=1.20 k2=1.20 IDEAL_MASK\")\n",
    "\n",
    "ax[1].imshow(res2,cmap='gray',vmax=170)\n",
    "ax[1].title.set_text(\"Inverse vmax=170 13° ITER D0=300 k1=0 k2=1.20 BUTTERWORTH_MASK\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = cv2.imread('pet.jpg',0)\n",
    "runHPTests(img,D0=5,mask_type=IDEAL_MASK)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = cv2.imread('pet.jpg',0)\n",
    "runHPTests(img,D0=300,mask_type=BUTTERWORTH_MASK)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = cv2.imread('pet.jpg',0)\n",
    "runHPTests(img,D0=0.05,mask_type=GAUSSIAN_MASK)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 6\n",
    "\n",
    "**Given the DFT translation property:**\n",
    "\n",
    "$f(x,y)e^{j2\\pi(u_0x/M + v_0y/N)} \\iff F(u-u_0,v-v_0)$ **and**\n",
    "\n",
    "$f(x - x_0,y - y_0) \\iff F(u,v)e^{j2\\pi(ux_x/M + vy_0/N)}$ **,**\n",
    "\n",
    "**show that** $f(x,y)(-1)^{x+y} \\iff F(u-M/2,v-N/2)$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As the book explains, a way to explain this equivalence is setting the following values $u_0 = M/2$ e $v_0 = N/2$ respectively\n",
    "\n",
    "\n",
    "$$f(x,y)e^{j2\\pi(u_0x/M + v_0y/N)} \\iff F(u-u_0,v-v_0)$$\n",
    "\n",
    "$$f(x,y)e^{j2\\pi(M/2x/M + N/2y/N)} \\iff F(u-M/2,v-N/2)$$\n",
    "\n",
    "So the exponential term going to be $e^{j\\pi x+y}$ that is the same of $(-1)^{x+y}$\n",
    "\n",
    "$f(x,y)(-1)^{x+y} \\iff F(u-M/2,v-N/2)$\n",
    "\n",
    "If we assume the values x = 0 and y = 0 for the image coordinates, we obtain the center of DFT:\n",
    "\n",
    "$f(x,y)(-1)^{x+y} \\iff F(u-M/2,v-N/2)$\n",
    "\n",
    "$f(0,0)(-1)^{0+0} \\iff F(0-M/2,0-N/2)$\n",
    "\n",
    "$f(0,0) \\iff F(M/2,N/2)$\n",
    "\n",
    "<br>\n",
    "\n",
    "If we assume the values x = 5 and y = 5 for a 10x10 image, we will be in the center of the image, however, in the DFT we are on the top left (0,0) position:\n",
    "\n",
    "\n",
    "$f(x,y)(-1)^{x+y} \\iff F(u-M/2, v-N/2)$\n",
    "\n",
    "$f(5,5)(-1)^{5+5} \\iff F(5-10/2, 5-10/2)$\n",
    "\n",
    "$f(5,5) \\iff F(5-10/2, 5-10/2)$\n",
    "\n",
    "$f(5,5) \\iff F(0,0)$\n",
    "\n",
    "That is, this is a way to reallocate the origin from matrix to your center. This procedure is called by *shift*, and is useful to analyze the spectrum of the image. When we calculate the DFT of one image the high values going to corners of spectrum, making it difficult to see the pattern, so we apply the shift, and we get a new image of spectrum where the values of corners going to center.\n",
    "\n",
    "Despite the question not asking for a code, I always like to realize some practice to better understand, so the next explanations refer to my practical tests.\n",
    "\n",
    "The next figure illustrates the movement of the origin of the matrix from the top left to the center."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 2)\n",
    "fig.set_size_inches(16, 7)\n",
    "\n",
    "def prepareAx(ax):\n",
    "    ax.spines[\"top\"].set_visible(False)\n",
    "    ax.spines[\"right\"].set_visible(False)\n",
    "    ax.spines['left'].set_position('center')\n",
    "    ax.spines['bottom'].set_position('center')\n",
    "    ax.invert_yaxis()\n",
    "    ax.set_xticks([x for x in range(-10,11,5)])\n",
    "    ax.set_yticks([x for x in range(-10,11,5)])\n",
    "    ax.grid(True)\n",
    "\n",
    "prepareAx(ax[0])\n",
    "prepareAx(ax[1])\n",
    "rect = patches.Rectangle((0, 0), 5, 5, linewidth=2, edgecolor='b', facecolor='none')\n",
    "ax[0].add_patch(rect)\n",
    "\n",
    "rect = patches.Rectangle((-2.5, -2.5), 5, 5, linewidth=2, edgecolor='b', facecolor='none')\n",
    "ax[1].add_patch(rect)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Practice\n",
    "\n",
    "In the next code I implemented two functions according to the equation, the first *shift(dft)* it's refer to $F(u-M/2,v-N/2)$ and the second *shiftBefore(img)* is $f(x,y)(-1)^{x+y}$. The first should be applied to the DFT and the second to the image before calculate the DFT."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def shift(dft):\n",
    "    M, N = dft.shape\n",
    "    new = np.zeros(dft.shape,dtype='complex')\n",
    "    for x in range(M):\n",
    "        for y in range(N):\n",
    "            new[x,y] = dft[math.ceil(x-M/2),math.ceil(y-N/2)]\n",
    "    return new\n",
    "\n",
    "def shiftBefore(img,dtype='complex'):\n",
    "    #new = np.zeros(img.shape,dtype=img.dtype)\n",
    "    new = np.zeros(img.shape,dtype=dtype)\n",
    "    for x in range(img.shape[0]):\n",
    "        for y in range(img.shape[1]):\n",
    "            new[x,y] = img[x, y] * ((-1)**(x+y))\n",
    "    return new\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then I created one matrix to observer the action of *shift* more closely. Note that with the use of the first function, the colors that are in the corners going to the center of the image. It's like the image was divided in four parts, and these parts was reallocated.\n",
    "\n",
    "The second function will invert the values of colors in certain positions, this affects the generated spectrum, causing one effect similar to the application of the first function that was applied directly on the spectrum."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mat = np.asarray([[6,0,0,0,0,7],\n",
    "                  [0,0,0,0,0,0],\n",
    "                  [1,0,3,5,0,1],\n",
    "                  [1,0,2,4,0,1],\n",
    "                  [0,0,0,0,0,0],\n",
    "                  [8,0,0,0,0,9]])\n",
    "        \n",
    "new = shift(mat).astype(int)\n",
    "\n",
    "fig, ax = plt.subplots(2, 2)\n",
    "fig.set_size_inches(10, 10)\n",
    "\n",
    "ax[0,0].imshow(mat, cmap='Paired')\n",
    "ax[0,0].title.set_text(\"Original\")\n",
    "ax[0,1].imshow(new, cmap='Paired')\n",
    "ax[0,1].title.set_text(\"Shifted\")\n",
    "\n",
    "\n",
    "ax[1,0].imshow(mat, cmap='Paired')\n",
    "ax[1,0].title.set_text(\"Original\")\n",
    "new = shiftBefore(mat).astype(int)\n",
    "ax[1,1].imshow(new, cmap='Paired')\n",
    "ax[1,1].title.set_text(\"Shift before\")\n",
    "\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the code below I present one example of the comparison of *shift* implemented by me and the Numpy shift, despite the implementation of the Numpy shift be different, both got the same result. The last image was the inverse of spectrum, where was applied my implementation of the shift."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 4)\n",
    "fig.set_size_inches(20, 6)\n",
    "\n",
    "img = cv2.imread('pet.jpg',0)\n",
    "#opencv shift\n",
    "F1 = fp.fft2((img).astype(float))\n",
    "F2 = fp.fftshift(F1)\n",
    "\n",
    "#my shift\n",
    "F2a = shift(F1)\n",
    "\n",
    "ax[0].title.set_text(\"Spectrum Before Shift\")\n",
    "ax[0].imshow(spectrum(F1), cmap='gray')\n",
    "\n",
    "ax[1].title.set_text(\"Spectrum OpenCV shift\")\n",
    "ax[1].imshow(spectrum(F2), cmap='gray')\n",
    "\n",
    "ax[2].title.set_text(\"Spectrum My shift\")\n",
    "ax[2].imshow(spectrum(F2a), cmap='gray')\n",
    "\n",
    "ax[3].title.set_text(\"Inverse of My shift\")\n",
    "ax[3].imshow(inverse(F2a), cmap='gray')\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next I demonstrate the effect of the second function, that is related to $f(x,y)^{x+y}$. The first is the original image, the second is the result of its application on the image, isn't possible identify anything, only a dotted texture, on the third image was applied $2 * log(img)$ to improve the visualization of the image, the fourth is the spectrum generated from this image and the fifth is the inverse of spectrum.\n",
    "\n",
    "Obs: Note that the spectrum is slightly different, in visual terms when compared to the previous example, it's like \"noiseless\", but the result of reconstruction is the same in both."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 5)\n",
    "fig.set_size_inches(20, 6)\n",
    "\n",
    "img = cv2.imread('pet.jpg',0)\n",
    "\n",
    "ax[0].title.set_text(\"Original Img\")\n",
    "ax[0].imshow(img, cmap='gray')\n",
    "\n",
    "\n",
    "img = shiftBefore(img,dtype='float')\n",
    "F1 = fp.fft2((img).astype(float))\n",
    "\n",
    "#imagem do jeito que ficou\n",
    "ax[1].title.set_text(\"Image after $(-1)^{x+y}$\")\n",
    "ax[1].imshow(img, cmap='gray')\n",
    "\n",
    "\n",
    "#(20*np.log10( 0.1 + mag)).astype(int)\n",
    "ax[2].title.set_text(\"Image after $2 * log((-1)^{x+y})$\")\n",
    "ax[2].imshow(2 * np.log(img).astype(int), cmap='gray')\n",
    "\n",
    "ax[3].title.set_text(\"Spectrum of image\")\n",
    "ax[3].imshow(spectrum(F1), cmap='gray')\n",
    "\n",
    "\n",
    "F1v = shiftBefore(fp.ifft2(F1).real).real\n",
    "\n",
    "ax[4].title.set_text(\"Inverse of spectrum\")\n",
    "ax[4].imshow(F1v, cmap='gray')\n",
    "\n",
    "\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 7\n",
    "\n",
    "<b>A professor of archeology doing research on currency exchange practices during the Roman Empire recently became aware that four Roman coins crucial to his research are listed in the holdings of the British Museum in London. Unfortunately, he was told after arriving there that the coins had been recently stolen. Further research on his part revealed that the museum keeps photographs of every item for which it is responsible. Unfortunately, the photos of the coins in question are blurred to the point where the date and other small markings are not readable. The cause of the blurring was the camera being out of focus when the pictures were taken. As an image processing expert and friend of the professor, you are asked as a favor to determine whether computer processing can be utilized to restore the images to the point where the professor can read the markings. You are told that the original camera used to take the photos is still available, as are other representative coins of the same era. Propose a step-by-step solution to this problem.</b>\n",
    "\n",
    "\n",
    "To solve this problem, first we need to obtain the degradation function. I had a lot of difficult to understand this procedure, because, according to Gonzalez, et. al. (2008) explain, if we have the equipment that was used to capture the degraded photos, we can obtain a more accurate degradation function estimate, by capturing several images until we get one capture with most similar effect that one that we want to recover. Once that configuration was found, the next step is to determine the Point Spread Function (PSF). In this point I first was understood that we should, realize the capture of a small white point using the same configuration, but after my practical tests, I believe I got the translation wrong, and in this case it's as if the PSF was the mask that used to degrade the image.\n",
    "\n",
    "But, once the degradation function is known, it's possible to apply the *Inverse Filter*, however, it's not indicated, because most of real cases, it couldn't have a good result if the degradation function values are low. The *Wiener filter* is most indicated.\n",
    "\n",
    "The author presents the following equation to obtain the degradation function $H(u,v)$:\n",
    "\n",
    "$H(u,v) = \\dfrac{G(u,v)}{A}$\n",
    "\n",
    "The $G(u,v)$ is the Fourier transform of the degraded image, and the $A$ is the strength of impulse, but I don't understand  how obtain the A value, and I didn't find examples. So I can't see the advantage of have the same equipment of original photo with this equation.\n",
    "\n",
    "But, when the author mentions the subject **Estimation by Image Observation** he presents the following equation:\n",
    "\n",
    "$H_s(u,v) = \\dfrac{G_s(u,v)}{\\hat{F}_s(u,v)}$\n",
    "\n",
    "In this case the different between the first equation and the second is on the divisor, $\\hat{F}_s(u,v)$, that is the Fourier transform of the same image with proper focus. The author explain that we can get part of image, preferentially  where exists some structure that we can enhance to improve the visualization, then we divide the Fourier transform of the degraded image to the Fourier of the enhanced image to obtain the degradation function.\n",
    "\n",
    "With this procedure I got a better result, and the use of the original camera started to make sense. In this case the procedure is:\n",
    "1. Take a photo of one coin where the degradation effect be close to the image that you want to restore.\n",
    "2. Take one photo of the same coin with a proper configuration of focus.\n",
    "3. Apply the equation above to get the degradation function.\n",
    "3. Apply the following equation $F(u,v) = \\dfrac{G(u,v)}{H(u,v)}$, to obtain the Fourier transform of the restored image.\n",
    "5. Apply the inverse of Fourier to the result of the previous equation to obtain the restored image.\n",
    "\n",
    "To get this conclusion the book reading wasn't enough, was necessary to realize practical experiments, next i presents the experiments that consist in:\n",
    "- Apply computationally blur and restore the image (Success)\n",
    "- Apply blur with smartphone camera and restore the image (Fail)\n",
    "- Apply the Wiener Filter\n",
    "    - In the degraded image artificially  (Success)\n",
    "    - In the degraded image from phone (Fail)\n",
    "- Apply the Richard Lucy\n",
    "    - In the degraded image artificially  (Success)\n",
    "    - In the degraded image from phone (Fail)\n",
    "   \n",
    "\n",
    "\n",
    "To this practice I adapted the code from: https://github.com/maponti/imageprocessing_course_icmc/blob/master/05b_restoration_deconvolution.ipynb\n",
    "\n",
    "\n",
    "## Experiment 1 - Apply computationally blur and restore the image\n",
    "### Apply blur on the images\n",
    "First, I applied the Gaussian blur on the 5c and 25c coins until the year is illegible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def gaussian_filter(k=5, sigma=1.0):\n",
    "    arx = np.arange((-k // 2) + 1.0, (k // 2) + 1.0)\n",
    "    x, y = np.meshgrid(arx, arx)\n",
    "    filt = np.exp(-(1/2) * (np.square(x) + np.square(y)) / np.square(sigma))\n",
    "    return filt / np.sum(filt)\n",
    "\n",
    "\n",
    "f = cv2.imread('coins/5_ok.jpg',0)\n",
    "h = gaussian_filter(k=35, sigma=10.5)\n",
    "\n",
    "\n",
    "#pega o h e deixa ele do tamanho da imagem onde será aplicado\n",
    "#adiciona zeros nas bordas até ficar do tamanho desejado\n",
    "# computing the number of padding on one side\n",
    "a = int(f.shape[0]//2 - h.shape[0]//2)\n",
    "h_pad = np.pad(h, (a,a-1), 'constant', constant_values=(0))\n",
    "\n",
    "\n",
    "# computing the Fourier transforms\n",
    "F = fftn(f)\n",
    "H = fftn(h_pad)\n",
    "\n",
    "plt.subplot(121)\n",
    "plt.imshow(fftshift(np.log(np.abs(F)+1)), cmap=\"gray\")\n",
    "plt.subplot(122)\n",
    "plt.imshow(fftshift(np.log(np.abs(H)+1)), cmap=\"gray\")\n",
    "\n",
    "# convolution - aplica o blur na imagem\n",
    "G = np.multiply(F,H)\n",
    "\n",
    "# Aplica o inverse\n",
    "# Inverse Transform\n",
    "# - we have to perform FFT shift before reconstructing the image in the space domain\n",
    "g = fftshift(ifftn(G).real)\n",
    "#g = cv2.blur(f,(25,25)) #Os dois métodos de aplicar blur geram o mesmo resultado\n",
    "\n",
    "fig, ax = plt.subplots(1, 3)\n",
    "fig.set_size_inches(20, 6)\n",
    "\n",
    "ax[0].imshow(f, cmap=\"gray\", vmin=0, vmax=255); \n",
    "ax[0].title.set_text(\"original image\")\n",
    "\n",
    "ax[1].imshow(g, cmap=\"gray\", vmin=0, vmax=255);\n",
    "ax[1].title.set_text(\"degraded/blurred image\")\n",
    "\n",
    "#Agora aplica o mesmo blur na de 25 cent\n",
    "f25 = cv2.imread('coins/25_ok.jpg',0)\n",
    "F25 = fftn(f25)\n",
    "G25 = np.multiply(F25,H)\n",
    "g25 = fftshift(ifftn(G25)).real\n",
    "\n",
    "ax[2].imshow(g25, cmap=\"gray\", vmin=0, vmax=255); \n",
    "ax[2].title.set_text(\"degraded/blurred image\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Estimating the degradation function\n",
    "\n",
    "From the 5c coin I can recover the degradation function, dividing the Fourier transform of the degraded image by the Fourier transform of the image with proper focus.\n",
    "\n",
    "That is, the following equation:\n",
    "\n",
    "$H_s(u,v) = \\dfrac{G_s(u,v)}{\\hat{F}_s(u,v)}$\n",
    "\n",
    "But, instead of apply only one part of the image I applied the function on the whole image. Then it should be disregarded the $_s$ on the terms of above equation.\n",
    "\n",
    "\n",
    "As soon as I get $H(u,v)$, I applied $\\hat{F}(u,v) = \\dfrac{G(u,v)}{H(u,v)}$ on the 5c coin that is degraded to create the Fourier transform of the image to be restored. Finally I apply the inverse to obtain the restored image.\n",
    "\n",
    "With this, I can recover the 5c coin."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#obtem a func de degradacao\n",
    "H_obtido = np.divide(G,F)\n",
    "#obtem a fourrier da imagem restaurada\n",
    "F_hat = np.divide(G,H_obtido)\n",
    "#obtem a imagem restaurada\n",
    "f_hat = ifftn(F_hat).real\n",
    "\n",
    "fig, ax = plt.subplots(1, 3)\n",
    "fig.set_size_inches(20, 6)\n",
    "\n",
    "ax[0].title.set_text(\"degraded/blurred image\")\n",
    "ax[0].imshow(g, cmap=\"gray\", vmin=0, vmax=255);\n",
    "\n",
    "ax[1].title.set_text(\"restored\")\n",
    "ax[1].imshow(f_hat, cmap=\"gray\"); plt.title(\"restored\")\n",
    "\n",
    "ax[2].title.set_text(\"Func degradação obtida\")\n",
    "ax[2].imshow(fftshift(np.log(np.abs(H)+1)), cmap=\"gray\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Restoring the 25c coin\n",
    "\n",
    "Now that the degradation function was obtained from the 5c coin, it's possible to restore the 25c coin."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#obtem a fourrier da imagem restaurada\n",
    "F_hat = np.divide(G25,H_obtido)\n",
    "#obtem a imagem restaurada\n",
    "f_hat = ifftn(F_hat).real\n",
    "\n",
    "\n",
    "plt.figure(figsize=(12,5))\n",
    "plt.subplot(121)\n",
    "plt.imshow(g25, cmap=\"gray\", vmin=0, vmax=255); plt.title(\"degraded/blurred image\")\n",
    "plt.subplot(122)\n",
    "plt.imshow(f_hat, cmap=\"gray\"); plt.title(\"restored\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiment 2 - Apply blur with smartphone camera and restore the image\n",
    "\n",
    "I tried to repeat the same procedure of experiment 1, however, instead of apply a blur computationally, I applied the blur at the moment that I was taking the photos. The next steps were the same.\n",
    "- Take a photo of the 5c coin with blur and without blur.\n",
    "- Take a photo of the 25c coin with blur.\n",
    "- Restore the 25c coin from the degradation function obtained from the 5c coin.\n",
    "\n",
    "I also tried to capture a white point focused and blurred to obtain the degradation function, but also without success, I only can restore the image that was used to create the degradation function.\n",
    "\n",
    "Unfortunately  I didn't get a good result in this experiment, even the degradation function it's very different from expected. (Previous example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = cv2.imread('coins/psfw_ok.jpg',0)\n",
    "g = cv2.imread('coins/psfw_blur.jpg',0)\n",
    "\n",
    "#f = cv2.imread('coins/5_ok.jpg',0)\n",
    "#g = cv2.imread('coins/5_blur.jpg',0)\n",
    "\n",
    "\n",
    "# computing the Fourier transforms\n",
    "F = fftn(f)\n",
    "G = fftn(g)\n",
    "#obtem a func de degradacao\n",
    "H = np.divide(G,F)\n",
    "H_bk = H\n",
    "\n",
    "#obtem a fourrier da imagem restaurada\n",
    "F_hat = np.divide(G,H)\n",
    "#obtem a imagem restaurada\n",
    "f_hat = ifftn(F_hat).real\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots(1, 3)\n",
    "fig.set_size_inches(20, 6)\n",
    "\n",
    "ax[0].title.set_text(\"degraded/blurred image\")\n",
    "ax[0].imshow(g, cmap=\"gray\");\n",
    "\n",
    "ax[1].title.set_text(\"Restored\")\n",
    "ax[1].imshow(f_hat, cmap=\"gray\");\n",
    "\n",
    "ax[2].title.set_text(\"Degradation function\")\n",
    "#ax[2].imshow(fftshift(np.log(np.abs(H)+1)), cmap=\"gray\");\n",
    "ax[2].imshow(spectrum(fftshift(H)), cmap=\"gray\");\n",
    "plt.show()\n",
    "\n",
    "\n",
    "g5b = cv2.imread('coins/5_blur.jpg',0)\n",
    "G5b = fftn(g5b)\n",
    "#obtem a fourrier da imagem restaurada\n",
    "F_hat = np.divide(G5b,H)\n",
    "#obtem a imagem restaurada\n",
    "f_hat = ifftn(F_hat).real\n",
    "\n",
    "\n",
    "plt.figure(figsize=(12,5))\n",
    "plt.subplot(121)\n",
    "plt.imshow(g5b, cmap=\"gray\"); plt.title(\"degraded/blurred image\")\n",
    "plt.subplot(122)\n",
    "plt.imshow(f_hat, cmap=\"gray\"); plt.title(\"restored\")\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n",
    "g25b = cv2.imread('coins/25_blur.jpg',0)\n",
    "G25b = fftn(g25b)\n",
    "#obtem a fourrier da imagem restaurada\n",
    "F_hat = np.divide(G25b,H)\n",
    "#obtem a imagem restaurada\n",
    "f_hat = ifftn(F_hat).real\n",
    "\n",
    "\n",
    "plt.figure(figsize=(12,5))\n",
    "plt.subplot(121)\n",
    "plt.imshow(g25b, cmap=\"gray\"); plt.title(\"degraded/blurred image\")\n",
    "plt.subplot(122)\n",
    "plt.imshow(f_hat, cmap=\"gray\"); plt.title(\"restored\")\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After some tests I was wondering if is possible to restore the same coin but with different images, so I tested:\n",
    "- Capture 5c coin with proper focus (A)\n",
    "- Capture 5c coin with blur (B)\n",
    "- Capture another photo like the B\n",
    "- Capture another photo like the B, but move the coin (D)\n",
    "- Capture another photo like the B, but with the coin rotated (E)\n",
    "\n",
    "The result is that I can't recover any photo, only that I used to get the degradation error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "f = cv2.imread('coins/5c/5c_ok.jpg',0)\n",
    "g = cv2.imread('coins/5c/5c_blur1.jpg',0)\n",
    "\n",
    "\n",
    "\n",
    "# computing the Fourier transforms\n",
    "F = fftn(f)\n",
    "G = fftn(g)\n",
    "#obtem a func de degradacao\n",
    "H = np.divide(G,F)\n",
    "\n",
    "#obtem a fourrier da imagem restaurada\n",
    "F_hat = np.divide(G,H)\n",
    "#obtem a imagem restaurada\n",
    "f_hat = ifftn(F_hat).real\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots(1, 3)\n",
    "fig.set_size_inches(20, 6)\n",
    "\n",
    "ax[0].title.set_text(\"degraded/blurred image (B)\")\n",
    "ax[0].imshow(g, cmap=\"gray\");\n",
    "\n",
    "ax[1].title.set_text(\"Restored (B)\")\n",
    "ax[1].imshow(f_hat, cmap=\"gray\");\n",
    "\n",
    "ax[2].title.set_text(\"Degradation function\")\n",
    "#ax[2].imshow(fftshift(np.log(np.abs(H)+1)), cmap=\"gray\");\n",
    "ax[2].imshow(spectrum(fftshift(H)), cmap=\"gray\");\n",
    "plt.show()\n",
    "\n",
    "\n",
    "g5b = cv2.imread('coins/5c/5c_blur2.jpg',0)\n",
    "G5b = fftn(g5b)\n",
    "#obtem a fourrier da imagem restaurada\n",
    "F_hat = np.divide(G5b,H)\n",
    "#obtem a imagem restaurada\n",
    "f_hat = ifftn(F_hat).real\n",
    "\n",
    "\n",
    "plt.figure(figsize=(12,5))\n",
    "plt.subplot(121)\n",
    "plt.imshow(g5b, cmap=\"gray\"); plt.title(\"degraded/blurred image (C)\")\n",
    "plt.subplot(122)\n",
    "plt.imshow(f_hat, cmap=\"gray\"); plt.title(\"restored (C)\")\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n",
    "g25b = cv2.imread('coins/5c/5c_blur3.jpg',0)\n",
    "G25b = fftn(g25b)\n",
    "#obtem a fourrier da imagem restaurada\n",
    "F_hat = np.divide(G25b,H)\n",
    "#obtem a imagem restaurada\n",
    "f_hat = ifftn(F_hat).real\n",
    "\n",
    "\n",
    "plt.figure(figsize=(12,5))\n",
    "plt.subplot(121)\n",
    "plt.imshow(g25b, cmap=\"gray\"); plt.title(\"degraded/blurred image (D)\")\n",
    "plt.subplot(122)\n",
    "plt.imshow(f_hat, cmap=\"gray\"); plt.title(\"restored (D)\")\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n",
    "g25b = cv2.imread('coins/5c/5c_blur4.jpg',0)\n",
    "G25b = fftn(g25b)\n",
    "#obtem a fourrier da imagem restaurada\n",
    "F_hat = np.divide(G25b,H)\n",
    "#obtem a imagem restaurada\n",
    "f_hat = ifftn(F_hat).real\n",
    "\n",
    "\n",
    "plt.figure(figsize=(12,5))\n",
    "plt.subplot(121)\n",
    "plt.imshow(g25b, cmap=\"gray\"); plt.title(\"degraded/blurred image (E)\")\n",
    "plt.subplot(122)\n",
    "plt.imshow(f_hat, cmap=\"gray\"); plt.title(\"restored (E)\")\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Others methods to restore\n",
    "\n",
    "I tried to apply others methods to restore the image using the same degradation function obtained  previously, also without success."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h = fftshift(H_bk).real\n",
    "\n",
    "res_img = rescale255(restoration.wiener(g25b, h, 0.1, clip=False))\n",
    "res_img2 = rescale255(restoration.richardson_lucy(img_as_float(g25b), h, 1))\n",
    "\n",
    "fig, ax = plt.subplots(1, 3)\n",
    "fig.set_size_inches(15, 6)\n",
    "\n",
    "ax[0].imshow(g25b,cmap='gray',vmin=0,vmax=255)\n",
    "ax[1].imshow(res_img,cmap='gray')\n",
    "ax[2].imshow(res_img2,cmap='gray')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Estimating the degradation function\n",
    "I tried to apply the *Inverse filter* with many configurations, also without success."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = cv2.imread('coins/5_blur.jpg',0)\n",
    "G = fftn(g)\n",
    "\n",
    "for k in [5,15,35,55,105,255]:\n",
    "    for sigma in np.arange(0.05,2.0,0.25):\n",
    "        h = gaussian_filter(k=k, sigma=sigma)\n",
    "\n",
    "        a = int(f.shape[0]//2 - h.shape[0]//2)\n",
    "        if h.shape[0] % 2 == 0:\n",
    "            h_pad = np.pad(h, (a,a), 'constant', constant_values=(0))\n",
    "        else:\n",
    "            h_pad = np.pad(h, (a,a-1), 'constant', constant_values=(0))\n",
    "        H = fftn(h_pad)\n",
    "\n",
    "        #F_hat = G/H\n",
    "        F_hat = np.divide(G,H)\n",
    "\n",
    "        #f_hat = ifftn(F_hat).real\n",
    "        f_hat = fftshift(ifftn(F_hat).real)\n",
    "\n",
    "\n",
    "        plt.figure(figsize=(12,5))\n",
    "        plt.subplot(121)\n",
    "        plt.imshow(g, cmap=\"gray\", vmin=0, vmax=255); plt.title(\"degraded/blurred image\")\n",
    "        plt.subplot(122)\n",
    "        plt.imshow(f_hat, cmap=\"gray\", vmin=0, vmax=255); plt.title(\"restored k=%d sigma=%.2f \"% (k,sigma))\n",
    "        plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wiener method\n",
    "### Applying the blur computationally and using the wiener filter\n",
    "\n",
    "As I didn't get good results with the *Inverse filter*, I decided to test others methods, before I applied the blur computationally and restored the image thought the *Wiener filter*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = cv2.imread('coins/25_ok.jpg',0)\n",
    "\n",
    "\n",
    "blur = cv2.blur(img,(25,25))\n",
    "\n",
    "bs = 25\n",
    "psf = np.ones((bs, bs)) / (bs*bs)\n",
    "res_img = rescale255(restoration.wiener(blur, psf, 0.1, clip=False))\n",
    "#res_img = rescale255(restoration.richardson_lucy(blur, psf, 1))\n",
    "\n",
    "fig, ax = plt.subplots(1, 2)\n",
    "fig.set_size_inches(15, 6)\n",
    "\n",
    "ax[0].imshow(blur,cmap='gray',vmin=0,vmax=255)\n",
    "ax[1].imshow(res_img,cmap='gray',vmin=0,vmax=255)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Restore the smartphone photo\n",
    "\n",
    "Still unsuccessful to restore the smartphone photo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "blur = cv2.imread('coins/25_blur.jpg',0)\n",
    "\n",
    "\n",
    "def testFloat(blurSize):\n",
    "    fig, ax = plt.subplots(1, 4)\n",
    "    fig.set_size_inches(20, 6)\n",
    "    i = 0\n",
    "    #for f in (start=start, stop=stop, num=5):\n",
    "    for f in [0.001,0.01,0.1,100]:\n",
    "        psf = np.ones((blurSize, blurSize)) / (blurSize*blurSize)\n",
    "        res_img = restoration.wiener(blur, psf, f, clip=False)\n",
    "\n",
    "        ax[i].title.set_text(\"Mask Size %d and %.5f\" % (blurSize,f))\n",
    "        ax[i].imshow(res_img,cmap='gray',vmax=255)\n",
    "        i += 1\n",
    "    plt.show()\n",
    "    \n",
    "for blurSize in range(3,13,2):\n",
    "    testFloat(blurSize=blurSize)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Richardson lucy method\n",
    "\n",
    "Also tried the *Richardson Lucy* method, this method can restore the computationally blur.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = cv2.imread('coins/25_ok.jpg',0)\n",
    "\n",
    "blur = cv2.blur(img,(25,25))\n",
    "\n",
    "bs = 25\n",
    "psf = np.ones((bs, bs)) / (bs*bs)\n",
    "res_img = rescale255(restoration.richardson_lucy(img_as_float(blur), psf, 35))\n",
    "\n",
    "fig, ax = plt.subplots(1, 2)\n",
    "fig.set_size_inches(15, 6)\n",
    "\n",
    "ax[0].imshow(blur,cmap='gray',vmin=0,vmax=255)\n",
    "ax[1].imshow(res_img,cmap='gray',vmin=0,vmax=255)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Smarpthone photo restore with Richardson Lucy\n",
    "\n",
    "As well as the Wienner method, this method can't restore the smartphone photo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#OBS ESTE MÉTODO É MAIS DEMORADO\n",
    "\n",
    "blur = img_as_float(cv2.imread('coins/25_blur.jpg',0))\n",
    "\n",
    "def testSizes():\n",
    "    \n",
    "    for it in range(30,81,20):\n",
    "        fig, ax = plt.subplots(1, 4)\n",
    "        fig.set_size_inches(20, 6)\n",
    "        i = 0\n",
    "        for blurSize in range(25,56,10):\n",
    "            \n",
    "            psf = np.ones((blurSize, blurSize)) / (blurSize*blurSize)\n",
    "            res_img = rescale255(restoration.richardson_lucy(blur, psf, it))\n",
    "\n",
    "            ax[i].title.set_text(\"Mask Size mask size=%d iteracoes=%d\" % (blurSize,it))\n",
    "            ax[i].imshow(res_img,cmap='gray')\n",
    "            i += 1\n",
    "        plt.show()\n",
    "\n",
    "testSizes()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 8\n",
    "\n",
    "**8. Describe which problem you will try to solve in your end of course project, which are the Image Processing sub-areas involved and which techniques you intend to use.**\n",
    "\n",
    "My course project is related to detection of malaria infection in blood smear. More precisely in segmentation and blood cells counting. The blood cells counting is important, because to determine the parasite density, it's required to have the most accurate possible count of blood cells. This project is based on the thesis of Oliveira (2019), so I pretend to realize the counting of blood cells using the same image bank of the original work and compare the results. The problem is that in the original work, some cells are so closely, that the author didn't separate the cells to count individually, instead, aa estimate was made based on area of the group of cells.\n",
    "The original work used:\n",
    "- HSV mask to detect the infection;\n",
    "- Adaptative OTSU, to to binarize the image, separating the background and the blood cells.\n",
    "- Hole filling by edge detection\n",
    "- Connected components\n",
    "- Erosion to separate components\n",
    "In my work, I pretend to test others possibilities listed by Wu, Merchant and Castleman (2010) like Watershed and Boundary-Based Segmentation instead OTSU, but I will need to use algorithms of original work like, Connected components, hole filling and erosion.\n",
    "\n",
    "The principal objective is to obtain a better blood cells segmentation.\n",
    "\n",
    "The specific objectives is:\n",
    "- Configure and run the code of original work to segment the images\n",
    "- Implement others methods listed to segment images\n",
    "- Compare segmentations methods in quality and performance\n",
    "\n",
    "\n",
    "**Bibliography**\n",
    "OLIVEIRA, Allisson Dantas de. MalariaApp: um sistema de baixo custo para diagnóstico de malária em lâminas de esfregaço sanguíneo usando dispositivos móveis. 2019.\n",
    "\n",
    "WU, Qiang; MERCHANT, Fatima; CASTLEMAN, Kenneth (Ed.). Microscope image processing. Elsevier, 2010."
   ]
  }
 ],
 "metadata": {
  "authors": [
   {
    "email": "klingerkrieg@gmail.com",
    "name": "Alan Klinger Sousa Alves"
   },
   {
    "name": "Prof. Bruno Motta"
   }
  ],
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "title": "Universidade Federal do Rio Grande do Norte - UFRN \n DIM0888 - PROCESSAMENTO DE IMAGENS \n Questions 1 to 4"
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
